---
permalink: /
title: "Laurence Aitchison"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Lecturer (US Assistant Professor) at the University of Bristol, developing a research program on Large Language Models (LLMs) that builds on my background in Bayesian machine learning and computational neuroscience.

## Research Journey

My research career began in computational neuroscience at the Gatsby Unit (UCL) and continued with a postdoc at Cambridge with Máté Lengyel. During this period, I published influential work using Bayesian inference to understand synaptic plasticity and neural activations, while also analyzing behavioral and neural data.

As a faculty member at Bristol, I developed an extensive research program in probabilistic and Bayesian machine learning. Key contributions include:

- Showing that infinite-width convolutional neural networks have Gaussian-process distributed outputs (300+ citations)
- Developing Deep Kernel Processes, a new family of probabilistic methods that combine the benefits of kernel methods with representation learning
- Advancing Bayesian neural networks through principled approaches to prior design and inference
- Resolving the longstanding "cold-posterior effect" puzzle in deep learning

## Current Focus: LLMs

I am now leveraging this unique combination of neuroscience and probabilistic ML expertise in the domain of LLMs. My work spans several critical challenges:

### Mechanistic Interpretability
Drawing on my neuroscience background, I study how LLMs process information - what I think of as the "Computational Neuroscience of LLMs." We're developing novel methods that go beyond simple interpretability to understand actual computation in these models.

### Bayesian Methods for LLMs
My expertise in Bayesian inference has proven particularly valuable for:
- Developing effective Bayesian approaches to LLM fine-tuning
- Creating uncertainty-aware methods to mitigate reward hacking
- Building rigorous evaluation frameworks for understanding LLM capabilities

### LLM Efficiency
I'm working on dramatically improving LLM performance and efficiency across the pipeline through:
- Differentiable token skipping for "boring" inputs
- Scale-free attention mechanisms
- Novel parameter sharing architectures
- Improved hyperparameter selection methods

## Teaching

I currently direct several courses at Bristol:
- Methods of AI (150 students)
- Data Driven Computer Science
- Computational Neuroscience

## Background

I hold degrees from the University of Cambridge in Physics (BA, Double First) and Systems Biology (MSci, First), followed by a PhD from the Gatsby Computational Neuroscience Unit at UCL.

