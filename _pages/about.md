---
permalink: /
title: "Laurence Aitchison"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm passionate about pushing the boundaries of large language model (LLM) research, with work spanning multiple exciting directions:
- **Pretraining Dynamics**: Uncovering the fundamental principles of how LLMs learn, through groundbreaking work on [hyperparameter transfer for weight decay](https://arxiv.org/abs/2405.13698) and [function-space learning rates](https://arxiv.org/abs/2502.17405)
- **Efficiency**: Developing methods to make LLMs more computationally efficient and accessible
- **Mechanistic Interpretability**: Breaking open the "black box" of LLMs through innovative approaches like [random baselines for SAEs](https://arxiv.org/abs/2502.18147), [residual stream analysis](https://arxiv.org/abs/2409.04185), and [Jacobian Sparse Autoencoders](https://arxiv.org/abs/2502.18147)
- **AI Agents**: Exploring the frontier of self-improving LLM systems

As a Lecturer (US Assistant Professor) at the University of Bristol, I lead research at the intersection of machine learning and artificial intelligence. While my current focus is on LLMs, my academic journey includes significant contributions to probabilistic and Bayesian machine learning, as well as computational neuroscience (PhD at the prestigious Gatsby Unit, UCL). For a deeper dive into my research trajectory, please see my CV or Publications.

## Let's Connect!

I'm always excited to discuss:
* Potential research collaborations
* PhD opportunities for motivated candidates
* Industry consulting partnerships

Reach out via email to start a conversation!

- Jan-March 2025: **New papers**
  - [Position: Don't use the CLT in LLM evals with fewer than a few hundred datapoints](https://arxiv.org/abs/2503.01747)
  - [Jacobian Sparse Autoencoders](https://arxiv.org/abs/2502.18147)
  - [Function-Space Learning Rates](https://arxiv.org/abs/2502.17405)
  - [Sparse Autoencoders Can Interpret Randomly Initialized Transformers](https://arxiv.org/abs/2501.17727)
- Jan 2025: **One paper accepted at ICLR 2025**: 
  - [Residual Stream Analysis with Multi-Layer SAEs](https://arxiv.org/abs/2409.04185)
- Nov 2024: **One paper accepted at 3DV 2025**:
  - [Snap-it, tap-it, splat-it: Tactile-informed 3D Gaussian splatting for reconstructing challenging surfaces](https://arxiv.org/abs/2403.20275)
- Sept 2024: **Two papers accepted at NeurIPS 2024**:
  - [Stochastic Kernel Regularisation Improves Generalisation in Deep Kernel Machines](https://arxiv.org/abs/2410.06171)
  - [Instruction Tuning With Loss Over Instructions](https://arxiv.org/abs/2405.14394)
- May 2024: **Three papers accepted at ICML 2024**:
  - [Bayesian low-rank adaptation for large language models](https://arxiv.org/abs/2308.13111)
  - [Convolutional Deep Kernel Machines](https://arxiv.org/abs/2309.09814)
  - [Position paper: Bayesian deep learning in the age of large-scale AI](https://arxiv.org/abs/2402.00809)
- May 2024: New paper: [How to set AdamWâ€™s weight decay as you scale model and dataset size](https://arxiv.org/abs/2405.13698)
